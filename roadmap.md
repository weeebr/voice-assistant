- [ ] **Phase 1: Foundations & Environment Setup ğŸ› ï¸ğŸ**

  - [ ] **Install system prerequisites**
    - [ ] Install Homebrew (if not already) ğŸº
    - [ ] Install Python 3.10+ via Homebrew (or use pyenv) ğŸ
    - [ ] Grant Terminal "Accessibility" rights in System Settings (for global hotkeys) ğŸ”‘
  - [ ] **Create isolated Python environment**
    - [ ] `python3 -m venv ~/.local-voice-env`
    - [ ] `source ~/.local-voice-env/bin/activate`
  - [ ] **Core library installations (pip) ğŸ“¦**
    - [ ] `pip install faster-whisper soundcard pvporcupine webrtcvad pynput`
    - [ ] _(Optional)_ `pip install llama-cpp-python transformers torch` for local LLM support ğŸ¤–
  - [ ] **Configure Appleâ€‘Silicon acceleration**
    - [ ] Verify PyTorch MPS backend works:
      ```bash
      python -c "import torch; print(torch.backends.mps.is_available())"
      ```
    - [ ] Set CTranslate2 to use Apple Neural Engine / Accelerate for Faster Whisper âš¡

- [ ] **Phase 2: Audio I/O & Lowâ€‘Latency Capture ğŸ™ï¸âš¡**

  - [ ] **SoundCard microphone streaming**
    - [ ] Identify default mic:
      ```python
      import soundcard as sc
      print(sc.default_microphone())
      ```
    - [ ] Open 16 kHz mono stream, small buffer (20 ms frames)
  - [ ] **Sample-rate conversion** (if needed)
    - [ ] Install `scipy` or `librosa` for resampling to 16 kHz
    - [ ] Integrate onâ€‘theâ€‘fly downsampling in audio loop
  - [ ] **Voice Activity Detection (VAD)**
    - [ ] Configure `webrtcvad.Vad(2)` for medium sensitivity
    - [ ] Feed 20 ms frames to VAD to detect endâ€‘ofâ€‘speech (trigger-mode)

- [ ] **Phase 3: Speechâ€‘toâ€‘Text Pipeline with Faster Whisper ğŸ¤–âœ¨**

  - [ ] **Model selection & download**
    - [ ] Choose model size: `tiny`/`base` for fastest, `small`/`medium` for balance
    - [ ] `from faster_whisper import WhisperModel; model = WhisperModel("small", device="cpu", compute_type="int8")`
  - [ ] **CTranslate2 / quantization settings**
    - [ ] Enable int8 quantization for speed
    - [ ] Set `beam_size=1` (greedy) to reduce latency
  - [ ] **Test realâ€‘time transcription**
    - [ ] Record short clip (2 s) â†’ transcribe â†’ measure endâ€‘toâ€‘end time < 2 s

- [ ] **Phase 4: Wakeâ€‘Word Detection (Triggerâ€‘Mode) ğŸ·ğŸ”‘**

  - [ ] **Picovoice Porcupine setup**
    - [ ] **Sign up** at Picovoice Console â†’ get `PICOVOICE_ACCESS_KEY` ğŸ”‘
    - [ ] Download / train `hey-assistant.ppn` keyword file via Console
  - [ ] **Integrate `pvporcupine`**
    - [ ] `import pvporcupine; pp = pvporcupine.create(access_key=..., keyword_paths=["hey-assistant.ppn"])`
    - [ ] Continuously read 512â€‘sample frames â†’ `pp.process(frame)` â†’ on match trigger STT
  - [ ] **Fallback / falseâ€‘positive mitigation**
    - [ ] Use VAD to confirm actual speech after wakeâ€‘word
    - [ ] Debounce wake callback (ignore additional triggers for 1 s)

- [ ] **Phase 5: Hotkeyâ€‘Based Pushâ€‘toâ€‘Talk ğŸ¹ğŸ”´**

  - [ ] **Global key listener with `pynput`**
    - [ ] `from pynput import keyboard` â†’ monitor `<cmd>` press & release
    - [ ] On `<cmd>` down â†’ start audio capture; on release â†’ stop & process
  - [ ] **Threading / async coordination**
    - [ ] Run key listener in separate thread to avoid blocking audio loop
    - [ ] Use threadâ€‘safe queue to pass raw audio chunks to STT thread

- [ ] **Phase 6: Postâ€‘Processing & Grammar Correction âœï¸âœ…**

  - [ ] **LanguageTool integration**
    - [ ] No API key needed; Java backend autoâ€‘downloads on first run
    - [ ] `import language_tool_python; tool_en = LanguageTool('en-US'); tool_de = LanguageTool('de-DE')`
    - [ ] `corrected = tool_en.correct(raw_text)` (choose tool based on detected language)
  - [ ] **Language detection (optional)**
    - [ ] If using Whisper's `language=` arg or run `langdetect.detect()` to pick correct tool
  - [ ] **Final formatting**
    - [ ] Normalize quotes: German â€â€¦â€œ vs English "â€¦"
    - [ ] Ensure proper capitalization & punctuation

- [ ] **Phase 7: (Optional) Local LLM for NLU/NLG ğŸ§ ğŸ’¬**

  - [ ] **Model & framework choice**
    - [ ] `llama-cpp-python` + LLaMA 2 (7B int4) with Metal acceleration
    - [ ] Or Hugging Face Transformers with `device="mps"`
  - [ ] **Download & quantize models**
    - [ ] If using HF: set `HUGGING_FACE_HUB_TOKEN` env var ğŸ”‘
    - [ ] `transformers-cli login` â†’ download model to `~/.cache/huggingface/`
  - [ ] **Inference integration**
    - [ ] Wrap transcription output â†’ pass as prompt to LLM
    - [ ] Stream / batch generate tokens for short responses

- [ ] **Phase 8: Orchestration & Architecture ğŸ—ï¸ğŸ“ˆ**

  - [ ] **Main process structure**
    - [ ] **Thread 1**: Wakeâ€‘word listener â†’ triggers recording
    - [ ] **Thread 2**: Hotkey listener â†’ triggers recording
    - [ ] **Thread 3**: Audio capture & VAD â†’ raw PCM frames â†’ buffer
    - [ ] **Thread 4**: STT & postâ€‘processing â†’ final text output
  - [ ] **Inter-thread communication**
    - [ ] Use `queue.Queue` for passing audio data and control events
    - [ ] Define clear event types: `WAKE_TRIGGER`, `HOTKEY_DOWN`, `HOTKEY_UP`, `AUDIO_READY`
  - [ ] **Configuration & logging**
    - [ ] Central `config.yaml` or `.env` for model paths, API keys, buffer sizes
    - [ ] Structured logging (`logging` module) for events, latencies, errors

- [ ] **Phase 9: Testing, Benchmarking & Optimization ğŸ”ğŸ“Š**

  - [ ] **Unit tests** for each component (wakeâ€‘word, hotkey, audio, STT, grammar)
  - [ ] **Integration tests**: simulate a full request from wake â†’ output
  - [ ] **Latency benchmarks**: record and log endâ€‘toâ€‘end times per mode
  - [ ] **Resource monitoring**: CPU, memory, thermal on M1 Pro â†’ optimize buffer sizes & threading
  - [ ] **Error handling & retries**: handle misfires, VAD misses, model load failures

- [ ] **Phase 10: Deployment & Packaging ğŸ“¦ğŸš€**
  - [ ] **CLI entrypoint**: `voice-assistant --mode=trigger|hotkey`
  - [ ] **Launch at login** (optional): create a Mac LaunchAgent plist
  - [ ] **Distribution**: package with `pyinstaller` or `shiv` for easy startup
  - [ ] **Documentation**: README with setup steps, key bindings, config options
